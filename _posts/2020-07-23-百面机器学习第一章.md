---
layout: article
title: 百面机器学习第一章
key: 100018
category: blog
tags: interview
date: 2020-07-23 14:16:00 +08:00
---

# 百面机器学习第一章: 特征工程

## 1. 特征归一化：为什么需要对数值型类型做归一化

常用的方法：

1. 线性函数归一化： $x_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}$
2. 零均值归一化：将原始数据映射到均值为0，标准差为1的分布上。$z=\frac{x-\mu}{\sigma}$

答案：

​	因为在随机梯度下降时，学习速率相同的情况下，不同尺度的特征更新速度不同（画图-从椭圆变成圆形），这会需要更多的迭代次数。

​	但是对于决策树模型不适用，决策树在节点分裂时主要依靠特征的信息增益比，这和归一化无关。

## 2. 类别型特征：在对数据进行预处理时，应该怎样处理类别型特征？

1. 序号编码

   ​	序号编码处理类别间具有大小关系的数据，（1，2，3）

2. One-hot

   ​	类别间不具有大小关系的特征。

   	1. 可使用稀疏向量节省空间（[0,0,0,0,1,0,3,0,0,0] -> [10,[4,6],[1,3]]）

    	2. 配合特征选择来降低维度

3. 二进制编码

   ​	将ID用二进制进行编码，节省存储空间

### 3. 高维组合特征的处理：什么是组合特征？如何处理高维组合特征？

1. 例如二阶特征：将一阶特征两两组合成为二阶特征。
2. 如果用户ID和物品ID数量较多，将用户和物品分别用低维向量进行标识。相当于矩阵分解。

### 4. 组合特征： 怎样有效地找到组合特征？

1. 使用决策树找到特征：采用梯度提升决策树，在之前决策树的残差上构建下一棵决策树。

### 5. 文本表示模型：有哪些文本表示模型？它们各有什么优缺点？

1. 词袋模型（Bag of words）

   忽略每个词出现的顺序，以一个长序列来表示一段文本，序列中的每一维反映了该词在原文的重要程度，常用TF-IDF来计算：${TF-IDF}(t,d) = TF(t,d)*IDF(t)$。TF为单词t在文档d中出现的频率。

   $IDF(t)=log{\frac{文章总数}{包含t的文章数+1}}$

   通常忽略了连续词组。可以将N-gram作为单独的特征放入单词表示，构成N-gram模型。

   同一个词会有多种词性变化，有相似的含义。实际 引用会对单词进行**词干提取**。

2. 主题模型

   用于从文本库发现有代表性的主题。得到每个主题词的分布特性。

3. 词嵌入与深度学习

   将单词映射成Dense Vector。

### 6. Word2Vec：Word2Vec是如何工作的？它和LDA有什么区别与联系？

Word2Vec又可分为CBOW和Skip-gram：

1. CBOW：根据上下文出现的词语来预测当前词的生成概率。
2. Skip-gram：根据当前词来预测上下文中各词的生成概率。
   1. 优化方法：
   2. Hierarchical Softmax
   3. Negative sampling

Word2Vec与LDA的区别和联系：

1. LDA是利用文档中的单词共现关系对单词按主题聚类，对“文档-单词”矩阵进行分解，Word2Vec是对“上下文-单词”矩阵进行学习。
2. 主题模型：基于概率图的生成式模型，词嵌入：神经网络，学习得到单词的稠密表示

